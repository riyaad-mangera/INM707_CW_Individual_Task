2024-05-10 19:23:59,462	WARNING algorithm_config.py:3959 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2024-05-10 19:23:59,495	WARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!
2024-05-10 19:24:01,348	INFO worker.py:1749 -- Started a local Ray instance.
[36m(RolloutWorker pid=1372)[39m 2024-05-10 19:24:07,240	WARNING deprecation.py:50 -- DeprecationWarning: `num_envs_per_worker` has been deprecated. Use `AlgorithmConfig.num_envs_per_env_runner` instead. This will raise an error in the future!
[36m(RolloutWorker pid=1372)[39m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[36m(RolloutWorker pid=1372)[39m [Powered by Stella]
[36m(RolloutWorker pid=1372)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=1372)[39m   logger.warn(
[36m(RolloutWorker pid=1372)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=1372)[39m   logger.warn(
[36m(RolloutWorker pid=1372)[39m Exception raised in creation task: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=1372, ip=127.0.0.1, actor_id=8403b70ef7a93b79fd2e5ae501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C9E3D5A3D0>)
[36m(RolloutWorker pid=1372)[39m   File "python\ray\_raylet.pyx", line 1887, in ray._raylet.execute_task
[36m(RolloutWorker pid=1372)[39m   File "python\ray\_raylet.pyx", line 1828, in ray._raylet.execute_task.function_executor
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\function_manager.py", line 691, in actor_method_executor
[36m(RolloutWorker pid=1372)[39m     return method(__ray_actor, *args, **kwargs)
[36m(RolloutWorker pid=1372)[39m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
[36m(RolloutWorker pid=1372)[39m     return method(self, *_args, **_kwargs)
[36m(RolloutWorker pid=1372)[39m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 532, in __init__
[36m(RolloutWorker pid=1372)[39m     self._update_policy_map(policy_dict=self.policy_dict)
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
[36m(RolloutWorker pid=1372)[39m     return method(self, *_args, **_kwargs)
[36m(RolloutWorker pid=1372)[39m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1714, in _update_policy_map
[36m(RolloutWorker pid=1372)[39m     updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)
[36m(RolloutWorker pid=1372)[39m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
[36m(RolloutWorker pid=1372)[39m     return method(self, *_args, **_kwargs)
[36m(RolloutWorker pid=1372)[39m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1792, in _get_complete_policy_specs_dict
[36m(RolloutWorker pid=1372)[39m     preprocessor = ModelCatalog.get_preprocessor_for_space(
[36m(RolloutWorker pid=1372)[39m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(RolloutWorker pid=1372)[39m   File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\models\catalog.py", line 886, in get_preprocessor_for_space
[36m(RolloutWorker pid=1372)[39m     raise Exception(
[36m(RolloutWorker pid=1372)[39m Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
2024-05-10 19:24:07,415	ERROR actor_manager.py:517 -- Ray error, taking actor 1 out of service. The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=1372, ip=127.0.0.1, actor_id=8403b70ef7a93b79fd2e5ae501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C9E3D5A3D0>)
  File "python\ray\_raylet.pyx", line 1887, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1828, in ray._raylet.execute_task.function_executor
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\function_manager.py", line 691, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 532, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1714, in _update_policy_map
    updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1792, in _get_complete_policy_specs_dict
    preprocessor = ModelCatalog.get_preprocessor_for_space(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\models\catalog.py", line 886, in get_preprocessor_for_space
    raise Exception(
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
2024-05-10 19:24:07,420	ERROR actor_manager.py:517 -- Ray error, taking actor 2 out of service. The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=21920, ip=127.0.0.1, actor_id=0065503640cea20ff6bb8abc01000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x0000019F2FEE2290>)
  File "python\ray\_raylet.pyx", line 1887, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1828, in ray._raylet.execute_task.function_executor
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\function_manager.py", line 691, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 532, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1714, in _update_policy_map
    updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1792, in _get_complete_policy_specs_dict
    preprocessor = ModelCatalog.get_preprocessor_for_space(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\models\catalog.py", line 886, in get_preprocessor_for_space
    raise Exception(
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 161, in __init__
    self._setup(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 231, in _setup
    self.add_workers(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 742, in add_workers
    raise result.get()
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\utils\actor_manager.py", line 497, in _fetch_result
    result = ray.get(r)
             ^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\worker.py", line 2623, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\worker.py", line 863, in get_objects
    raise value
ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=1372, ip=127.0.0.1, actor_id=8403b70ef7a93b79fd2e5ae501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C9E3D5A3D0>)
  File "python\ray\_raylet.pyx", line 1887, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1828, in ray._raylet.execute_task.function_executor
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\function_manager.py", line 691, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 532, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1714, in _update_policy_map
    updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1792, in _get_complete_policy_specs_dict
    preprocessor = ModelCatalog.get_preprocessor_for_space(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\models\catalog.py", line 886, in get_preprocessor_for_space
    raise Exception(
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\individual_task_riyaad\main.py", line 77, in <module>
    algo = config.build()
           ^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm_config.py", line 857, in build
    return algo_class(
           ^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 558, in __init__
    super().__init__(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\trainable\trainable.py", line 158, in __init__
    self.setup(copy.deepcopy(self.config))
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 644, in setup
    self.workers = WorkerSet(
                   ^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 183, in __init__
    raise e.args[0].args[2]
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 161, in __init__
    self._setup(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 231, in _setup
    self.add_workers(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 742, in add_workers
    raise result.get()
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\utils\actor_manager.py", line 497, in _fetch_result
    result = ray.get(r)
             ^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\worker.py", line 2623, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\worker.py", line 863, in get_objects
    raise value
ray.exceptions.ActorDiedError: The actor died because of an error raised in its creation task, [36mray::RolloutWorker.__init__()[39m (pid=1372, ip=127.0.0.1, actor_id=8403b70ef7a93b79fd2e5ae501000000, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x000001C9E3D5A3D0>)
  File "python\ray\_raylet.pyx", line 1887, in ray._raylet.execute_task
  File "python\ray\_raylet.pyx", line 1828, in ray._raylet.execute_task.function_executor
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\_private\function_manager.py", line 691, in actor_method_executor
    return method(__ray_actor, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 532, in __init__
    self._update_policy_map(policy_dict=self.policy_dict)
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1714, in _update_policy_map
    updated_policy_dict = self._get_complete_policy_specs_dict(policy_dict)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\util\tracing\tracing_helper.py", line 467, in _resume_span
    return method(self, *_args, **_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\rollout_worker.py", line 1792, in _get_complete_policy_specs_dict
    preprocessor = ModelCatalog.get_preprocessor_for_space(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\models\catalog.py", line 886, in get_preprocessor_for_space
    raise Exception(
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']
During handling of the above exception, another exception occurred:
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\individual_task_riyaad\main.py", line 77, in <module>
    algo = config.build()
           ^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm_config.py", line 857, in build
    return algo_class(
           ^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 558, in __init__
    super().__init__(
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\trainable\trainable.py", line 158, in __init__
    self.setup(copy.deepcopy(self.config))
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 644, in setup
    self.workers = WorkerSet(
                   ^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\evaluation\worker_set.py", line 183, in __init__
    raise e.args[0].args[2]
Exception: Unknown config key `env_config`, all keys: ['_disable_preprocessor_api', '_disable_action_flattening', 'fcnet_hiddens', 'fcnet_activation', 'fcnet_weights_initializer', 'fcnet_weights_initializer_config', 'fcnet_bias_initializer', 'fcnet_bias_initializer_config', 'conv_filters', 'conv_activation', 'conv_kernel_initializer', 'conv_kernel_initializer_config', 'conv_bias_initializer', 'conv_bias_initializer_config', 'conv_transpose_kernel_initializer', 'conv_transpose_kernel_initializer_config', 'conv_transpose_bias_initializer', 'conv_transpose_bias_initializer_config', 'post_fcnet_hiddens', 'post_fcnet_activation', 'post_fcnet_weights_initializer', 'post_fcnet_weights_initializer_config', 'post_fcnet_bias_initializer', 'post_fcnet_bias_initializer_config', 'free_log_std', 'no_final_linear', 'vf_share_layers', 'use_lstm', 'max_seq_len', 'lstm_cell_size', 'lstm_use_prev_action', 'lstm_use_prev_reward', 'lstm_weights_initializer', 'lstm_weights_initializer_config', 'lstm_bias_initializer', 'lstm_bias_initializer_config', '_time_major', 'use_attention', 'attention_num_transformer_units', 'attention_dim', 'attention_num_heads', 'attention_head_dim', 'attention_memory_inference', 'attention_memory_training', 'attention_position_wise_mlp_dim', 'attention_init_gru_gate_bias', 'attention_use_n_prev_actions', 'attention_use_n_prev_rewards', 'framestack', 'dim', 'grayscale', 'zero_mean', 'custom_model', 'custom_model_config', 'custom_action_dist', 'custom_preprocessor', 'encoder_latent_dim', 'always_check_shapes', 'lstm_use_prev_action_reward', '_use_default_native_models']