2024-05-10 21:46:28,436	WARNING algorithm_config.py:3959 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.
2024-05-10 21:46:28,436	WARNING deprecation.py:50 -- DeprecationWarning: `num_envs_per_worker` has been deprecated. Use `AlgorithmConfig.num_envs_per_env_runner` instead. This will raise an error in the future!
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py:525: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
`UnifiedLogger` will be removed in Ray 2.7.
  return UnifiedLogger(config, logdir, loggers=None)
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\tune\logger\unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS="ignore::DeprecationWarning"
The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.
  self._loggers.append(cls(self.config, self.logdir, self.trial))
2024-05-10 21:46:28,523	WARNING deprecation.py:50 -- DeprecationWarning: `max_num_worker_restarts` has been deprecated. Use `AlgorithmConfig.max_num_env_runner_restarts` instead. This will raise an error in the future!
2024-05-10 21:46:32,106	INFO worker.py:1749 -- Started a local Ray instance.
[36m(RolloutWorker pid=36104)[39m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)
[36m(RolloutWorker pid=36104)[39m [Powered by Stella]
[36m(RolloutWorker pid=36104)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=36104)[39m   logger.warn(
[36m(RolloutWorker pid=36104)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=36104)[39m   logger.warn(
[36m(RolloutWorker pid=36120)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\torch\nn\modules\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
[36m(RolloutWorker pid=36120)[39m   return F.conv2d(input, weight, bias, self.stride,
2024-05-10 21:46:58,983	WARNING algorithm_config.py:3959 -- You have specified 1 evaluation workers, but your `evaluation_interval` is 0 or None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.
[36m(RolloutWorker pid=25304)[39m A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
[36m(RolloutWorker pid=25304)[39m [Powered by Stella][32m [repeated 2x across cluster]
[36m(RolloutWorker pid=36120)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=36120)[39m   logger.warn([32m [repeated 2x across cluster]
[36m(RolloutWorker pid=36120)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=36104)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\torch\nn\modules\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
[36m(RolloutWorker pid=36104)[39m   return F.conv2d(input, weight, bias, self.stride,
[36m(RolloutWorker pid=25304)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=25304)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\gymnasium\core.py:311: UserWarning: [33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.
[36m(RolloutWorker pid=25304)[39m C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\torch\nn\modules\conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ..\aten\src\ATen\native\cudnn\Conv_v8.cpp:919.)
[36m(RolloutWorker pid=25304)[39m   return F.conv2d(input, weight, bias, self.stride,
2024-05-10 21:47:12,330	INFO trainable.py:161 -- Trainable.setup took 43.808 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.
2024-05-10 21:47:12,332	WARNING util.py:61 -- Install gputil for GPU system monitoring.
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\individual_task_riyaad\main.py", line 156, in <module>
    algo = Algorithm.from_checkpoint('./checkpoints\\algorithm_state.pkl')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 323, in from_checkpoint
    raise ValueError(
ValueError: Cannot restore a v0 checkpoint using `Algorithm.from_checkpoint()`!In this case, do the following:
1) Create a new Algorithm object using your original config.
2) Call the `restore()` method of this algo object passing it your checkpoint dir or AIR Checkpoint object.
Traceback (most recent call last):
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\individual_task_riyaad\main.py", line 156, in <module>
    algo = Algorithm.from_checkpoint('./checkpoints\\algorithm_state.pkl')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\riyaa\Documents\Uni Stuff\Masters\Semester 2\Subjects\Deep Reinforcement Learning\CW_Individual\INM707_CW_Individual_Task\INM707_CW_env\Lib\site-packages\ray\rllib\algorithms\algorithm.py", line 323, in from_checkpoint
    raise ValueError(
ValueError: Cannot restore a v0 checkpoint using `Algorithm.from_checkpoint()`!In this case, do the following:
1) Create a new Algorithm object using your original config.
2) Call the `restore()` method of this algo object passing it your checkpoint dir or AIR Checkpoint object.